{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8621667,"sourceType":"datasetVersion","datasetId":5161079}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:31:30.908850Z","iopub.execute_input":"2024-06-06T14:31:30.909872Z","iopub.status.idle":"2024-06-06T14:31:44.682956Z","shell.execute_reply.started":"2024-06-06T14:31:30.909837Z","shell.execute_reply":"2024-06-06T14:31:44.681878Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nCollecting pycocotools\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom sklearn.cluster import DBSCAN\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:31:49.069407Z","iopub.execute_input":"2024-06-06T14:31:49.070090Z","iopub.status.idle":"2024-06-06T14:31:54.638665Z","shell.execute_reply.started":"2024-06-06T14:31:49.070055Z","shell.execute_reply":"2024-06-06T14:31:54.637727Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class DhakaTrafficDataset(Dataset):\n    def __init__(self, root, annotation, transform=None):\n        self.root = root\n        self.coco = COCO(annotation)\n        self.ids = list(self.coco.imgs.keys())\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        anns = coco.loadAnns(ann_ids)\n        img_metadata = coco.loadImgs(img_id)[0]\n        path = os.path.join(self.root, img_metadata['file_name'])\n\n        image = Image.open(path).convert(\"RGB\")\n        boxes = []\n        labels = []\n\n        for ann in anns:\n            xmin = ann['bbox'][0]\n            ymin = ann['bbox'][1]\n            xmax = xmin + ann['bbox'][2]\n            ymax = ymin + ann['bbox'][3]\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(ann['category_id'])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, target\n\n# Define transforms with data augmentation\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create datasets\ntrain_dataset = DhakaTrafficDataset(\n    root=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/train\",\n    annotation=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/train/_combined_annotations.coco.json\",\n    transform=transform\n)\n\nval_dataset = DhakaTrafficDataset(\n    root=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/valid\",\n    annotation=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/valid/_combined_annotations.coco.json\",\n    transform=transform\n)\n\ntest_dataset = DhakaTrafficDataset(\n    root=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/test\",\n    annotation=\"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/test/_combined_annotations.coco.json\",\n    transform=transform\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:31:58.028449Z","iopub.execute_input":"2024-06-06T14:31:58.028904Z","iopub.status.idle":"2024-06-06T14:32:00.089523Z","shell.execute_reply.started":"2024-06-06T14:31:58.028874Z","shell.execute_reply":"2024-06-06T14:32:00.088654Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=1.31s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.18s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.39s)\ncreating index...\nindex created!\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# Load a pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Replace the head of the model for the number of classes in the dataset\nnum_classes = 10  # 9 object classes + 1 background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Move model to the device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Define optimizer and learning rate scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T14:32:03.268708Z","iopub.execute_input":"2024-06-06T14:32:03.269061Z","iopub.status.idle":"2024-06-06T14:32:05.675995Z","shell.execute_reply.started":"2024-06-06T14:32:03.269034Z","shell.execute_reply":"2024-06-06T14:32:05.675044Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 158MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    total_loss = 0\n    for images, targets in data_loader:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        losses.backward()\n        optimizer.step()\n        \n        total_loss += losses.item()\n\n    avg_loss = total_loss / len(data_loader)\n    print(f\"Epoch {epoch} Average Loss: {avg_loss}\")\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n\n            # Ensure loss_dict is a dictionary\n            if isinstance(loss_dict, dict):\n                losses = sum(loss for loss in loss_dict.values())\n                total_loss += losses.item()\n            else:\n                # Handle case where loss_dict is not as expected\n                print(\"Unexpected loss_dict format:\", loss_dict)\n    \n    avg_loss = total_loss / len(data_loader)\n    return avg_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T15:56:57.928327Z","iopub.execute_input":"2024-06-06T15:56:57.929206Z","iopub.status.idle":"2024-06-06T15:56:57.939624Z","shell.execute_reply.started":"2024-06-06T15:56:57.929172Z","shell.execute_reply":"2024-06-06T15:56:57.938769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Function to verify data loader\ndef verify_data_loader(data_loader, coco_annotation_file):\n    # Load COCO annotations from the provided file\n    with open(coco_annotation_file, 'r') as f:\n        coco_data = json.load(f)\n    \n    # Get annotations for the first image from the JSON file\n    image_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == 0]\n    \n    for images, targets in data_loader:\n        # Extract the first image and its annotations\n        image = images[0]\n        target = targets[0]\n        \n        # Print the shape of the image and the targets\n        print(\"Image Shape:\", image.shape)\n        print(\"Targets:\", target)\n        \n        # Compare the targets with annotations from the JSON file\n        for i, bbox in enumerate(target['boxes']):\n            if i < len(image_annotations):\n                coco_bbox = image_annotations[i]['bbox']\n                coco_label = image_annotations[i]['category_id']\n                \n                print(\"Predicted BBox:\", bbox)\n                print(\"COCO BBox:\", coco_bbox)\n                print(\"Predicted Label:\", target['labels'][i])\n                print(\"COCO Label:\", coco_label)\n                print()\n            else:\n                print(\"No more annotations in COCO data\")\n                break\n        break\n\n# Verify the data loader for the first image\nverify_data_loader(train_loader, \"/kaggle/input/dhaka-city-traffic-2024/Final_Dhaka_Traffic_Dataset/train/_combined_annotations.coco.json\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)\n#     val_loss = evaluate(model, val_loader, device)\n#     print(f\"Epoch {epoch} Validation Loss: {val_loss}\")\n    lr_scheduler.step()\n    torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')\n\n# Save the final model\ntorch.save(model.state_dict(), 'faster_rcnn_dhaka_traffic_final.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T15:57:03.576703Z","iopub.execute_input":"2024-06-06T15:57:03.577429Z","iopub.status.idle":"2024-06-07T02:06:05.483683Z","shell.execute_reply.started":"2024-06-06T15:57:03.577397Z","shell.execute_reply":"2024-06-07T02:06:05.482670Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 0 Average Loss: 0.5877125271998639\nEpoch 1 Average Loss: 0.5449087815969271\nEpoch 2 Average Loss: 0.5063790909002441\nEpoch 3 Average Loss: 0.4060862099662358\nEpoch 4 Average Loss: 0.3733894415362433\nEpoch 5 Average Loss: 0.3500240932596953\nEpoch 6 Average Loss: 0.3246603087322811\nEpoch 7 Average Loss: 0.3210463280256789\nEpoch 8 Average Loss: 0.3180305777711379\nEpoch 9 Average Loss: 0.31519344845160385\nEpoch 10 Average Loss: 0.3145954629495805\nEpoch 11 Average Loss: 0.31449836255844243\nEpoch 12 Average Loss: 0.3138036439642635\nEpoch 13 Average Loss: 0.313688265851101\nEpoch 14 Average Loss: 0.313716301393561\nEpoch 15 Average Loss: 0.31372223408917155\nEpoch 16 Average Loss: 0.31361268756008304\nEpoch 17 Average Loss: 0.31347067068497686\nEpoch 18 Average Loss: 0.3138335367428814\nEpoch 19 Average Loss: 0.3140278331056953\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the saved model\nmodel_path = 'faster_rcnn_dhaka_traffic_final.pth'\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\nnum_classes = 10  # Adjust based on your dataset\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)\n\n# Define optimizer and learning rate scheduler\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n# Continue training for additional epochs\nnum_epochs_additional = 10\nfor epoch in range(num_epochs_additional):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)\n    lr_scheduler.step()\n    torch.save(model.state_dict(), f'model_epoch_{epoch + num_epochs}.pth')\n\n# Save the final model\ntorch.save(model.state_dict(), 'faster_rcnn_dhaka_traffic_final_updated.pth')\n","metadata":{},"execution_count":null,"outputs":[]}]}